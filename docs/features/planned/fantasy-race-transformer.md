# Fantasy Race Transformation & Local Image Generation Platform

**Status**: Planning Phase
**Priority**: High Value - Unlocks Multiple Feature Sets
**Estimated Timeline**: 4-6 weeks
**Dependencies**: RTX 4090 PC setup, network configuration

---

## Executive Summary

This initiative encompasses two synergistic capabilities that will fundamentally expand Life-OS's image generation capabilities:

1. **Fantasy Race Transformation**: Transform photos of real people into D&D fantasy races (elves, orcs, dwarves, tieflings, etc.) while maintaining recognizable features and personal identity
2. **Local Image Generation Infrastructure**: Integrate ComfyUI running on RTX 4090 GPU to unlock advanced techniques (IP-Adapter-FaceID, ControlNet, Flux.1, custom LoRAs) not available via cloud APIs

**Why This Matters**:
- **D&D Character Creation**: Players can generate character portraits that actually look like themselves
- **Cost Reduction**: $0 per image (vs. $0.04-$0.08 with cloud APIs) for high-volume generation
- **Quality Enhancement**: Access to state-of-the-art identity preservation techniques (17-18% improvement in face similarity)
- **Creative Control**: Fine-grained control over transformations, custom models, and artistic styles
- **Platform Foundation**: Local generation infrastructure enables future features (video, animation, inpainting, face swapping)

---

## Core Concept: Identity-Preserving Fantasy Transformations

### The Problem

D&D players want character portraits that:
- Actually look like them (not generic fantasy art)
- Transform them into their chosen race (elf, orc, dwarf, etc.)
- Maintain recognizable features (bone structure, eyes, facial proportions)
- Look like "me as an elf", not "a random elf"

**Current Limitations**:
- Text-only prompts (DALL-E 3) cannot preserve identity without reference images
- Cloud APIs lack access to specialized face-preservation models
- Generic fantasy art doesn't create personal connection to characters

### The Solution

**Identity-Preserving Transformation Pipeline**:
```
Subject Photo ‚Üí Face Analysis ‚Üí Fantasy Race Template ‚Üí Local Generation (IP-Adapter-FaceID) ‚Üí Character Portrait
     ‚Üì                ‚Üì                    ‚Üì                          ‚Üì
 Real person    Bone structure      Race features          "You as an elf"
                 Eye spacing         (pointed ears,         maintaining your
                 Face shape          skin tone shift)       recognizable features
```

**Key Innovation**: Separate identity preservation (structural features) from fantasy transformation (additive features), ensuring the person remains recognizable while gaining fantasy characteristics.

---

## Fantasy Race Transformation System

### Supported Races (Phase 1)

#### Core Races

**1. Elves**
- **Visual Features**:
  - Pointed ears (2-3 inches, slight backward curve)
  - Delicate, refined facial structure
  - Enhanced cheekbones, elegant jawline
  - Larger almond-shaped eyes
  - Ethereal quality to skin

- **Subraces**:
  - **High Elf**: Pale, luminous skin; aristocratic bearing; 3-inch ears
  - **Wood Elf**: Warm earthy tones; nature-attuned features; 2-inch ears
  - **Dark Elf (Drow)**: Deep purple/grey skin; white/silver hair; red/white eyes

- **Identity Preservation Priority**: HIGH (subtle transformation)
- **Transformation Intensity**: 20-40% (mostly additive features)

**2. Orcs / Half-Orcs**
- **Visual Features**:
  - Small to medium tusks from lower jaw
  - Pronounced brow ridge
  - Broader nose, square jaw
  - Muscular facial structure
  - Greenish-grey skin tone (maintaining undertones)

- **Subraces**:
  - **Full Orc**: Green skin, larger tusks, heavy scarring
  - **Half-Orc**: Mixed features, subtle tusks, grey-green skin

- **Identity Preservation Priority**: MEDIUM (moderate structural changes)
- **Transformation Intensity**: 50-70%

**3. Dwarves**
- **Visual Features**:
  - Stocky, broad facial structure
  - Prominent facial hair (all genders)
  - Strong, pronounced features
  - Ruddy complexion
  - Weathered appearance

- **Subraces**:
  - **Mountain Dwarf**: Robust build, red/brown hair
  - **Hill Dwarf**: Warmer skin tones, keen eyes
  - **Gray Dwarf (Duergar)**: Grey/purple skin, white hair

- **Identity Preservation Priority**: MEDIUM-HIGH
- **Transformation Intensity**: 40-60%

**4. Tieflings**
- **Visual Features**:
  - Horns (ram-style curved OR pointed - varies by bloodline)
  - Devil-like tail
  - Solid-color eyes (no pupil/iris distinction)
  - Red-range skin tones (official) or any color (fan/Wildemount)
  - Optional: fangs, clawed fingers

- **Subraces by Bloodline**:
  - **Asmodeus**: Large curved horns, deep red skin
  - **Zariel**: Flaming aesthetic, sharp features
  - **Levistus**: Ice-blue tones, crystalline horns

- **Identity Preservation Priority**: MEDIUM (supernatural features)
- **Transformation Intensity**: 60-80%

**5. Dragonborn** (Phase 2)
- **Visual Features**:
  - Draconic head structure with humanoid eyes/expression
  - Scaled skin texture
  - Color matching draconic ancestry (red, gold, blue, etc.)
  - No hair (head crest or horns instead)

- **Identity Preservation Priority**: LOW (extreme transformation)
- **Transformation Intensity**: 80-95%
- **Challenge**: Requires advanced facial structure morphing

#### Expansion Races (Phase 3)

- **Genasi** (elemental-touched): Fire, water, air, earth variants
- **Aasimar** (celestial heritage): Glowing eyes, ethereal features, optional wings
- **Halflings**: Scaled-down features, youthful appearance
- **Gnomes**: Small stature, exaggerated features, vibrant hair colors
- **Tabaxi** (cat people): Feline features while maintaining face structure
- **Kenku** (bird people): Avian features, feathered appearance

### Transformation System Architecture

#### Intensity Slider (0-100%)
Controls balance between identity preservation and fantasy transformation:

- **0-20% (Subtle Hints)**: Minimal changes, mostly cosmetic
  - Example: Slightly pointed ear tips, hint of different skin undertone
  - Use case: "Real world" fantasy, subtle supernatural hints

- **30-50% (Moderate)**: Clear fantasy features, highly recognizable
  - Example: Full elf ears, shifted skin tone, enhanced features
  - Use case: Standard D&D character portraits

- **60-80% (Strong)**: Dramatic transformation, still identifiable
  - Example: Full tiefling with horns/tail, orc with tusks/green skin
  - Use case: Distinctive fantasy characters

- **90-100% (Extreme)**: Maximum fantasy, structural changes
  - Example: Dragonborn with scaled reptilian features
  - Use case: Non-humanoid races

#### Identity Preservation Hierarchy

**Priority 1: Structural Features (ALWAYS MAINTAIN)**
1. Bone structure (facial shape, cheekbones, jawline)
2. Eye characteristics (spacing, shape, size, color)
3. Nose geometry (bridge width, tip shape, nostril shape)
4. Mouth proportions (lip fullness, cupid's bow, width)
5. Facial symmetry (left-right balance)
6. Feature proportions (golden ratio relationships)

**Priority 2: Secondary Features (PRESERVE WHEN POSSIBLE)**
7. Skin tone undertones (can shift hue but maintain warmth/coolness)
8. Expression muscles (how smile forms, brow furrows)
9. Distinctive marks (moles, freckles, scars - unless intentionally altered)
10. Gaze direction (where eyes look, eye contact patterns)

**Priority 3: Transformable Features (SAFE TO CHANGE)**
11. Skin color/texture (for fantasy races)
12. Ear shape/size (elf ears, pointed tips)
13. Facial hair (dwarf beards, orc tusks)
14. Supernatural additions (horns, glowing eyes, fangs)
15. Hair color/style (fantasy colors, elven flowing hair)

#### Prompt Engineering Pattern

```
SYSTEM INSTRUCTIONS:
Look at the reference image. Transform this person into a D&D [RACE].

üé® CRITICAL IDENTITY PRESERVATION (HIGHEST PRIORITY):
- Maintain exact face structure: bone structure, facial proportions
- Preserve eye spacing, nose shape, mouth proportions EXACTLY
- Keep all distinctive facial features that make this person recognizable
- Person MUST be clearly identifiable as themselves

üßù [RACE] TRANSFORMATION (PRIORITY 2):
[Race-specific features from template]
- Ears: [specific description]
- Skin: [color shift while maintaining undertones]
- Features: [additive fantasy elements]
- Special: [race-specific supernatural features]

Transformation Intensity: [X]%
(100% = full fantasy transformation, 50% = subtle hints, 0% = minimal changes)

CRITICAL BALANCE: The result should look like "[person's name] as a [race]",
NOT a generic [race]. Identity preservation overrides fantasy features if conflict occurs.

üìã PRESERVED FEATURES FROM REFERENCE:
- Facial bone structure: [extracted from analysis]
- Eye characteristics: [shape, spacing, color]
- Nose structure: [bridge, tip, width]
- Mouth shape: [lip fullness, proportions]
- Distinctive features: [any notable characteristics]

STYLE DIRECTION: [fantasy_art_style]
- Art style: [D&D official art / painterly / realistic / etc.]
- Lighting: [heroic / dramatic / natural]
- Framing: [portrait / headshot / bust]
```

### Race Template System

Each race has a structured template defining transformation parameters:

```python
# Example: High Elf Template
{
  "race_name": "High Elf",
  "race_category": "elf",
  "base_intensity": 0.4,  # Default transformation strength

  "structural_changes": {
    "cheekbones": "subtle enhancement (5-10% more defined)",
    "jawline": "slight refinement (more elegant, -5% width)",
    "brow_ridge": "minimal smoothing",
    "face_shape": "minimal change (preserve exact shape)"
  },

  "additive_features": {
    "ears": {
      "description": "Long pointed ears (3 inches above head)",
      "shape": "Elegant curve, slightly swept back",
      "prominence": "high"
    },
    "skin": {
      "tone_shift": "pale, luminous quality",
      "preserve_undertones": true,
      "special_effects": "subtle ethereal glow"
    },
    "eyes": {
      "size_adjustment": "+10% (slightly larger)",
      "shape_refinement": "more almond-shaped",
      "special_effects": "subtle mystical depth"
    }
  },

  "artistic_direction": {
    "overall_aesthetic": "Ethereal, aristocratic, elegant",
    "age_adjustment": "appear 10% younger (elven grace)",
    "expression": "maintain original expression",
    "lighting": "soft, flattering, slightly magical"
  },

  "forbidden_changes": [
    "Do NOT change facial bone structure",
    "Do NOT alter eye spacing or nose proportions",
    "Do NOT make unrecognizable",
    "Do NOT add facial hair unless present in original"
  ]
}
```

### Multi-Shot Reference Approach (Phase 2)

For enhanced identity preservation, support multiple reference photos:

1. **Front-facing portrait** (primary reference)
2. **45-degree angle** (captures facial depth)
3. **Profile view** (nose/jaw structure)
4. **Expression variety** (smiling, neutral, serious)
5. **Different lighting** (helps extract underlying structure)

**Workflow**:
```
Multiple Photos ‚Üí Face Feature Extraction ‚Üí Averaged Facial Structure ‚Üí Fantasy Transformation
                      ‚Üì
                 Common Features Analysis:
                 - Consistent eye shape across angles
                 - Nose structure from profile
                 - Face shape from multiple perspectives
                 - Expression patterns
```

**Benefits**:
- 17-18% improvement in identity preservation (research-backed)
- More robust to lighting/angle variations
- Better handling of distinctive features

---

## Local Image Generation Infrastructure

### Hardware Architecture

#### Network Topology

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Mac Studio (192.168.1.50)                                   ‚îÇ
‚îÇ - Life-OS Docker Stack (FastAPI, Frontend, PostgreSQL)      ‚îÇ
‚îÇ - Orchestrates all AI operations                            ‚îÇ
‚îÇ - Routes to cloud (Gemini/DALL-E) or local (ComfyUI)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚îÇ Gigabit Ethernet
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RTX 4090 PC (192.168.1.100)                                 ‚îÇ
‚îÇ - Windows 11 + WSL2 OR Linux (Ubuntu 24.04)                 ‚îÇ
‚îÇ - ComfyUI Docker Container (headless, API-only)             ‚îÇ
‚îÇ - NVIDIA Container Toolkit                                  ‚îÇ
‚îÇ - GPU: RTX 4090 (24GB VRAM)                                 ‚îÇ
‚îÇ - Storage: 500GB+ NVMe (models + output)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### RTX 4090 Specifications

**Why RTX 4090 is Ideal**:
- **VRAM**: 24GB (handles SDXL + IP-Adapter-FaceID + ControlNet simultaneously)
- **Performance**: 82.6 TFLOPS FP16 (5-10x faster than cloud for local models)
- **Architecture**: Ada Lovelace with 4th-gen Tensor Cores
- **Memory Bandwidth**: 1008 GB/s
- **Power**: 450W TDP (ensure adequate PSU: 850W+ recommended)

**Benchmark Performance**:
- **SDXL Base**: ~15-20 seconds (20 steps, 1024x1024)
- **Flux.1 Dev**: ~25-35 seconds (20 steps, 1024x1024)
- **IP-Adapter-FaceID + SDXL**: ~18-25 seconds
- **ControlNet + SDXL**: ~20-28 seconds

**Cost Analysis**:
- Hardware: ~$1,600 (RTX 4090)
- Electricity: ~$0.10/hour (450W at $0.15/kWh)
- **Break-even**: ~400 images vs. Gemini, ~200 images vs. DALL-E 3

### ComfyUI Setup & Configuration

#### Docker Deployment

**Container Configuration**:
```yaml
# docker-compose.yml (on RTX 4090 PC)
version: '3.8'

services:
  comfyui:
    image: mmartial/comfyui-nvidia-docker:ubuntu24_cuda12.6.3-latest
    container_name: comfyui-server
    runtime: nvidia

    ports:
      - "0.0.0.0:8188:8188"  # Expose to network

    volumes:
      # Persistent data
      - ./run:/comfy/mnt              # Virtual env, source code
      - ./basedir:/basedir            # User files, models
      - ./workflows:/workflows        # Workflow templates (API format)
      - ./output:/basedir/output      # Generated images
      - ./models:/basedir/models      # Model storage (~30-50GB)

    environment:
      # User permissions (match host user)
      - WANTED_UID=1000  # Run: id -u
      - WANTED_GID=1000  # Run: id -g

      # Base directory
      - BASE_DIRECTORY=/basedir

      # Security
      - SECURITY_LEVEL=normal

      # GPU
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 32G  # Prevent runaway memory usage

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
```

#### Essential Models & Storage

**Model Directory Structure**:
```
basedir/models/
‚îú‚îÄ‚îÄ checkpoints/                    # Base models (~7GB each)
‚îÇ   ‚îú‚îÄ‚îÄ sd_xl_base_1.0.safetensors
‚îÇ   ‚îú‚îÄ‚îÄ sd_xl_refiner_1.0.safetensors
‚îÇ   ‚îî‚îÄ‚îÄ flux1-dev.safetensors
‚îÇ
‚îú‚îÄ‚îÄ ipadapter/                      # IP-Adapter models (~500MB each)
‚îÇ   ‚îú‚îÄ‚îÄ ip-adapter-faceid-plusv2_sdxl.bin
‚îÇ   ‚îú‚îÄ‚îÄ ip-adapter-faceid_sdxl.bin
‚îÇ   ‚îî‚îÄ‚îÄ ip-adapter_sdxl_vit-h.safetensors
‚îÇ
‚îú‚îÄ‚îÄ loras/                          # LoRA fine-tunes (~100-500MB each)
‚îÇ   ‚îú‚îÄ‚îÄ dnd_fantasy_art_style.safetensors
‚îÇ   ‚îú‚îÄ‚îÄ realistic_character_portraits.safetensors
‚îÇ   ‚îî‚îÄ‚îÄ elf_features_v2.safetensors
‚îÇ
‚îú‚îÄ‚îÄ controlnet/                     # ControlNet models (~1-2GB each)
‚îÇ   ‚îú‚îÄ‚îÄ control_v11p_sd15_openpose.pth
‚îÇ   ‚îú‚îÄ‚îÄ control_v11f1p_sd15_depth.pth
‚îÇ   ‚îî‚îÄ‚îÄ controlnet-canny-sdxl-1.0.safetensors
‚îÇ
‚îú‚îÄ‚îÄ insightface/                    # Face recognition (~300MB)
‚îÇ   ‚îî‚îÄ‚îÄ models/antelopev2/
‚îÇ       ‚îú‚îÄ‚îÄ 1k3d68.onnx
‚îÇ       ‚îú‚îÄ‚îÄ 2d106det.onnx
‚îÇ       ‚îú‚îÄ‚îÄ genderage.onnx
‚îÇ       ‚îî‚îÄ‚îÄ scrfd_10g_bnkps.onnx
‚îÇ
‚îú‚îÄ‚îÄ clip_vision/                    # CLIP vision encoders (~1-2GB)
‚îÇ   ‚îî‚îÄ‚îÄ CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors
‚îÇ
‚îú‚îÄ‚îÄ vae/                            # VAE models (~300MB each)
‚îÇ   ‚îî‚îÄ‚îÄ sdxl_vae.safetensors
‚îÇ
‚îî‚îÄ‚îÄ upscale_models/                 # Upscalers (~50-500MB each)
    ‚îú‚îÄ‚îÄ 4x_NMKD-Superscale-SP_178000_G.pth
    ‚îî‚îÄ‚îÄ RealESRGAN_x4plus.pth
```

**Total Storage Required**: ~50-100GB depending on model selection

**Model Download Scripts**:
```bash
#!/bin/bash
# download_models.sh - Run on RTX 4090 PC

cd ~/comfyui/basedir/models

# SDXL Base
mkdir -p checkpoints && cd checkpoints
wget https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors

# IP-Adapter FaceID
cd ~/comfyui/basedir/models
mkdir -p ipadapter && cd ipadapter
wget https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid-plusv2_sdxl.bin
wget https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid_sdxl.bin

# InsightFace (for FaceID)
cd ~/comfyui/basedir/models
mkdir -p insightface/models
cd insightface/models
git clone https://huggingface.co/DIAMONIK7777/antelopev2
mv antelopev2 ./

# CLIP Vision
cd ~/comfyui/basedir/models
mkdir -p clip_vision && cd clip_vision
wget https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors
mv model.safetensors CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors

echo "Model download complete. Storage used:"
du -sh ~/comfyui/basedir/models
```

#### ComfyUI Custom Nodes (Required)

**Essential Plugins**:
```bash
cd ~/comfyui/basedir/custom_nodes

# IP-Adapter Plus (official)
git clone https://github.com/cubiq/ComfyUI_IPAdapter_plus.git

# Essentials pack
git clone https://github.com/cubiq/ComfyUI_essentials.git

# ControlNet Preprocessors
git clone https://github.com/Fannovel16/comfyui_controlnet_aux.git

# FaceDetailer (for face refinement)
git clone https://github.com/ltdrdata/ComfyUI-Impact-Pack.git
```

**Node Installation Verification**:
```bash
# Inside ComfyUI container
docker exec -it comfyui-server bash
cd /basedir/custom_nodes
ls -la
# Should see: ComfyUI_IPAdapter_plus, ComfyUI_essentials, etc.
```

### ComfyUI API Architecture

#### Native REST API Endpoints

ComfyUI includes built-in REST and WebSocket APIs:

**Core Endpoints**:
```
GET  /system_stats          # System info, GPU stats, available models
POST /prompt                # Queue workflow for execution
GET  /history               # List all job history
GET  /history/{prompt_id}   # Get specific job result
GET  /queue                 # Current queue status
POST /queue                 # Modify queue (delete, clear, etc.)
GET  /view                  # Download generated image
GET  /object_info           # Get all available nodes and their inputs
WS   /ws                    # WebSocket for real-time progress updates
```

**Example API Calls**:

1. **Queue a workflow**:
```bash
curl -X POST http://192.168.1.100:8188/prompt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": {
      "3": {
        "inputs": {
          "seed": 42,
          "steps": 20,
          "cfg": 7.0,
          "sampler_name": "euler",
          "scheduler": "normal",
          "denoise": 1.0,
          "model": ["4", 0],
          "positive": ["6", 0],
          "negative": ["7", 0],
          "latent_image": ["5", 0]
        },
        "class_type": "KSampler"
      },
      ...
    },
    "client_id": "life-os-12345"
  }'

# Response:
{
  "prompt_id": "abc-def-123",
  "number": 5,
  "node_errors": {}
}
```

2. **Check job status**:
```bash
curl http://192.168.1.100:8188/history/abc-def-123

# Response (when complete):
{
  "abc-def-123": {
    "prompt": [...],
    "outputs": {
      "9": {
        "images": [
          {
            "filename": "ComfyUI_00001_.png",
            "subfolder": "",
            "type": "output"
          }
        ]
      }
    }
  }
}
```

3. **Download generated image**:
```bash
curl "http://192.168.1.100:8188/view?filename=ComfyUI_00001_.png&type=output" \
  -o result.png
```

#### Workflow JSON Format (API vs. UI)

**CRITICAL**: ComfyUI has two JSON formats:

**UI Format** (`workflow.json`):
- Includes node positions (x, y coordinates)
- Human-readable for editor
- NOT suitable for API calls

**API Format** (`workflow_api.json`):
- Only includes node definitions and connections
- Used for programmatic execution
- Export via: Settings ‚Üí Dev mode Options ‚Üí Save (API Format)

**Example API Format Workflow**:
```json
{
  "3": {
    "inputs": {
      "seed": 42,
      "steps": 20,
      "cfg": 7.0,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1.0,
      "model": ["4", 0],
      "positive": ["6", 0],
      "negative": ["7", 0],
      "latent_image": ["5", 0]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "4": {
    "inputs": {
      "ckpt_name": "sd_xl_base_1.0.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "5": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage"
  },
  "6": {
    "inputs": {
      "text": "portrait of a person as a high elf, detailed fantasy art",
      "clip": ["4", 1]
    },
    "class_type": "CLIPTextEncode"
  },
  "7": {
    "inputs": {
      "text": "low quality, blurry, deformed",
      "clip": ["4", 1]
    },
    "class_type": "CLIPTextEncode"
  },
  "8": {
    "inputs": {
      "samples": ["3", 0],
      "vae": ["4", 2]
    },
    "class_type": "VAEDecode"
  },
  "9": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": ["8", 0]
    },
    "class_type": "SaveImage"
  }
}
```

**Node ID References**: `["4", 0]` means "output 0 from node 4"

### Python Client Integration

#### ComfyUI Workflow Client Library

**Installation**:
```bash
# In Life-OS api/requirements.txt
comfyui-workflow-client==0.1.5
```

**Basic Usage**:
```python
from comfyuiclient import ComfyUIClientAsync
import asyncio

async def generate_image():
    # Initialize client
    client = ComfyUIClientAsync(
        "192.168.1.100:8188",
        "workflows/ip_adapter_faceid.json"
    )
    await client.connect()

    # Set parameters (override workflow defaults)
    await client.set_data(key='KSampler', seed=12345, steps=25, cfg=7.5)
    await client.set_data(
        key='CLIP Text Encode Positive',
        text="portrait of a person as a high elf, detailed fantasy art"
    )

    # Generate and retrieve images
    results = await client.generate(["SaveImage"])  # Node name to retrieve

    # Save results
    for key, image in results.items():
        image.save(f"output/{key}.png")

    await client.close()

asyncio.run(generate_image())
```

**Features**:
- Automatic workflow format conversion (UI ‚Üí API)
- Async/sync support
- PIL Image object handling
- Debug mode with detailed logging
- Dynamic workflow reload

#### Custom ComfyUIService Wrapper

**Life-OS Integration** (`api/services/comfyui_service.py`):

```python
import asyncio
import aiohttp
import httpx
import json
import uuid
import time
from pathlib import Path
from typing import Optional, Dict, Any, Union, List
from api.logging_config import get_logger

logger = get_logger(__name__)


class ComfyUIService:
    """
    Service for interacting with ComfyUI API on RTX 4090 PC

    Handles:
    - Health checks and availability monitoring
    - Workflow template loading and parameter substitution
    - Job queuing and progress tracking
    - Image retrieval and caching
    - Error handling and fallback to cloud providers
    """

    def __init__(
        self,
        comfyui_url: str = None,
        timeout: int = 300,
        workflows_dir: Path = None
    ):
        """
        Initialize ComfyUI service

        Args:
            comfyui_url: Base URL (default: from env COMFYUI_URL)
            timeout: Max wait time for job completion (seconds)
            workflows_dir: Directory containing workflow templates
        """
        import os

        self.comfyui_url = comfyui_url or os.getenv(
            "COMFYUI_URL",
            "http://192.168.1.100:8188"
        )
        self.timeout = timeout
        self.workflows_dir = workflows_dir or Path("/app/api/workflows/comfyui")

        # Health check caching
        self._last_health_check = 0
        self._is_healthy = False
        self._health_check_interval = 30  # seconds

        # Workflow template cache
        self._workflow_cache = {}

        logger.info(f"ComfyUIService initialized: {self.comfyui_url}")

    async def is_healthy(self) -> bool:
        """
        Check if ComfyUI is available and responsive

        Caches result for 30 seconds to avoid excessive polling

        Returns:
            True if ComfyUI is available, False otherwise
        """
        # Return cached result if recent
        if time.time() - self._last_health_check < self._health_check_interval:
            return self._is_healthy

        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.comfyui_url}/system_stats")
                self._is_healthy = response.status_code == 200
                self._last_health_check = time.time()

                if self._is_healthy:
                    stats = response.json()
                    logger.info(
                        f"ComfyUI health check: HEALTHY "
                        f"(VRAM: {stats.get('system', {}).get('vram_used', 'N/A')})"
                    )
                else:
                    logger.warning(f"ComfyUI health check: UNHEALTHY (status {response.status_code})")

                return self._is_healthy

        except Exception as e:
            logger.warning(f"ComfyUI health check failed: {e}")
            self._is_healthy = False
            self._last_health_check = time.time()
            return False

    def load_workflow_template(self, workflow_name: str) -> Dict[str, Any]:
        """
        Load workflow template from JSON file

        Templates are cached in memory after first load

        Args:
            workflow_name: Name of workflow (e.g., "ip_adapter_faceid")

        Returns:
            Workflow JSON in API format
        """
        # Check cache
        if workflow_name in self._workflow_cache:
            return self._workflow_cache[workflow_name]

        # Load from file
        workflow_path = self.workflows_dir / f"{workflow_name}.json"

        if not workflow_path.exists():
            raise FileNotFoundError(f"Workflow template not found: {workflow_path}")

        with open(workflow_path, 'r') as f:
            workflow = json.load(f)

        # Cache for future use
        self._workflow_cache[workflow_name] = workflow
        logger.info(f"Loaded workflow template: {workflow_name}")

        return workflow

    def apply_parameters(
        self,
        workflow: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Apply parameter overrides to workflow template

        This is workflow-specific. Common mappings:
        - "seed" ‚Üí KSampler node
        - "prompt" / "positive_prompt" ‚Üí CLIP Text Encode (positive)
        - "negative_prompt" ‚Üí CLIP Text Encode (negative)
        - "steps" ‚Üí KSampler node
        - "cfg" / "guidance_scale" ‚Üí KSampler node
        - "width", "height" ‚Üí EmptyLatentImage node

        Args:
            workflow: Base workflow template
            parameters: Parameter overrides

        Returns:
            Modified workflow with parameters applied
        """
        # Deep copy to avoid mutating cache
        workflow = json.loads(json.dumps(workflow))

        # Iterate through all nodes
        for node_id, node_data in workflow.items():
            if "inputs" not in node_data:
                continue

            class_type = node_data.get("class_type", "")
            inputs = node_data["inputs"]

            # KSampler node
            if class_type == "KSampler":
                if "seed" in parameters:
                    inputs["seed"] = parameters["seed"]
                if "steps" in parameters:
                    inputs["steps"] = parameters["steps"]
                if "cfg" in parameters:
                    inputs["cfg"] = parameters["cfg"]
                elif "guidance_scale" in parameters:
                    inputs["cfg"] = parameters["guidance_scale"]
                if "sampler_name" in parameters:
                    inputs["sampler_name"] = parameters["sampler_name"]
                if "scheduler" in parameters:
                    inputs["scheduler"] = parameters["scheduler"]
                if "denoise" in parameters:
                    inputs["denoise"] = parameters["denoise"]

            # CLIP Text Encode nodes
            elif class_type == "CLIPTextEncode":
                # Positive prompt
                if "text" in inputs and inputs.get("text", "").startswith("portrait"):
                    if "prompt" in parameters:
                        inputs["text"] = parameters["prompt"]
                    elif "positive_prompt" in parameters:
                        inputs["text"] = parameters["positive_prompt"]

                # Negative prompt
                elif "text" in inputs:
                    if "negative_prompt" in parameters:
                        inputs["text"] = parameters["negative_prompt"]

            # Empty Latent Image (resolution)
            elif class_type == "EmptyLatentImage":
                if "width" in parameters:
                    inputs["width"] = parameters["width"]
                if "height" in parameters:
                    inputs["height"] = parameters["height"]

            # Load Image (subject image)
            elif class_type == "LoadImage":
                if "image_path" in parameters:
                    # Note: This requires the image to be in ComfyUI's input directory
                    # or accessible via network path
                    inputs["image"] = Path(parameters["image_path"]).name

        return workflow

    async def queue_prompt(
        self,
        workflow: Dict[str, Any],
        client_id: Optional[str] = None
    ) -> str:
        """
        Queue workflow for execution

        Args:
            workflow: Workflow JSON (with parameters applied)
            client_id: Optional client ID for tracking

        Returns:
            Prompt ID (job ID)
        """
        client_id = client_id or str(uuid.uuid4())

        payload = {
            "prompt": workflow,
            "client_id": client_id
        }

        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.comfyui_url}/prompt",
                json=payload
            )
            response.raise_for_status()
            result = response.json()

            prompt_id = result["prompt_id"]
            queue_number = result.get("number", "unknown")

            logger.info(
                f"ComfyUI job queued: {prompt_id} "
                f"(queue position: {queue_number})"
            )

            return prompt_id

    async def wait_for_completion(
        self,
        prompt_id: str,
        poll_interval: float = 1.0
    ) -> Dict[str, Any]:
        """
        Poll for job completion

        Args:
            prompt_id: Job ID from queue_prompt()
            poll_interval: Seconds between status checks

        Returns:
            Job history entry with outputs
        """
        start_time = time.time()

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            while True:
                # Check timeout
                elapsed = time.time() - start_time
                if elapsed > self.timeout:
                    raise TimeoutError(
                        f"ComfyUI job {prompt_id} timed out after {self.timeout}s"
                    )

                # Get job history
                response = await client.get(f"{self.comfyui_url}/history/{prompt_id}")
                response.raise_for_status()
                history = response.json()

                # Check if job completed
                if prompt_id in history:
                    job = history[prompt_id]

                    # Check for outputs (indicates completion)
                    if "outputs" in job:
                        logger.info(
                            f"ComfyUI job completed: {prompt_id} "
                            f"(duration: {elapsed:.1f}s)"
                        )
                        return job

                # Still running, wait before next poll
                await asyncio.sleep(poll_interval)

    async def get_output_image(self, job: Dict[str, Any]) -> bytes:
        """
        Extract and download output image from completed job

        Args:
            job: Job history entry from wait_for_completion()

        Returns:
            Image bytes
        """
        # Find output image in job results
        for node_id, node_output in job["outputs"].items():
            if "images" in node_output:
                image_info = node_output["images"][0]
                filename = image_info["filename"]
                subfolder = image_info.get("subfolder", "")
                file_type = image_info.get("type", "output")

                # Download image
                params = {
                    "filename": filename,
                    "subfolder": subfolder,
                    "type": file_type
                }

                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(
                        f"{self.comfyui_url}/view",
                        params=params
                    )
                    response.raise_for_status()

                    logger.info(
                        f"Downloaded output image: {filename} "
                        f"({len(response.content) / 1024:.1f} KB)"
                    )

                    return response.content

        raise ValueError("No output image found in job results")

    async def generate_image(
        self,
        workflow_name: str,
        parameters: Dict[str, Any],
        client_id: Optional[str] = None
    ) -> bytes:
        """
        High-level generation method

        Combines: load template ‚Üí apply parameters ‚Üí queue ‚Üí wait ‚Üí download

        Args:
            workflow_name: Name of workflow template
            parameters: Parameter overrides
            client_id: Optional client ID

        Returns:
            Image bytes
        """
        # Check health
        if not await self.is_healthy():
            raise Exception("ComfyUI is not available")

        # Load and prepare workflow
        workflow = self.load_workflow_template(workflow_name)
        workflow = self.apply_parameters(workflow, parameters)

        # Queue job
        prompt_id = await self.queue_prompt(workflow, client_id)

        # Wait for completion
        job = await self.wait_for_completion(prompt_id)

        # Download result
        return await self.get_output_image(job)
```

---

## Life-OS Integration Architecture

### Provider Routing Enhancement

#### LLMRouter Extension

**Modified**: `ai_tools/shared/router.py`

Add ComfyUI provider to existing routing logic:

```python
# At top of file
import os
from pathlib import Path
from typing import Optional, Union
from api.services.comfyui_service import ComfyUIService

class LLMRouter:
    """
    Enhanced LLM Router with multi-provider support

    Providers:
    - gemini: Gemini 2.5 Flash Image (cloud, multimodal)
    - dalle: DALL-E 3 (cloud, artistic)
    - comfyui: Local ComfyUI (RTX 4090, advanced techniques)
    """

    def __init__(self, ...):
        # ... existing initialization ...

        # ComfyUI service (lazy initialization)
        self._comfyui_service = None

    @property
    def comfyui_service(self) -> ComfyUIService:
        """Lazy-load ComfyUI service"""
        if self._comfyui_service is None:
            self._comfyui_service = ComfyUIService()
        return self._comfyui_service

    async def select_provider(
        self,
        provider: str,
        model: str,
        require_reference_image: bool = False
    ) -> str:
        """
        Auto-select best available provider

        Priority logic:
        1. If provider explicitly specified ‚Üí use that (may fail)
        2. If model is local-only (flux, sdxl+faceid) ‚Üí require ComfyUI
        3. If ComfyUI healthy and cost-sensitive ‚Üí prefer local
        4. Otherwise ‚Üí cloud (Gemini or DALL-E)

        Args:
            provider: "auto", "gemini", "dalle", "comfyui"
            model: Model name
            require_reference_image: If True, exclude DALL-E

        Returns:
            Selected provider name
        """
        # Explicit provider
        if provider != "auto":
            return provider

        # Model-specific requirements
        local_only_models = ["flux-dev", "flux-schnell", "sdxl-faceid"]
        if any(m in model.lower() for m in local_only_models):
            if await self.comfyui_service.is_healthy():
                return "comfyui"
            else:
                raise Exception(
                    f"Model {model} requires ComfyUI, but ComfyUI is unavailable"
                )

        # Prefer local if available (cost savings)
        if model.startswith("sdxl") or model.startswith("flux"):
            if await self.comfyui_service.is_healthy():
                logger.info(f"Auto-selecting ComfyUI for {model} (local generation)")
                return "comfyui"

        # Cloud fallback
        if require_reference_image:
            return "gemini"  # DALL-E doesn't support reference images
        else:
            return "gemini"  # Default to Gemini

    async def agenerate_image(
        self,
        prompt: str,
        image_path: Optional[Union[str, Path]] = None,
        model: str = "gemini-2.5-flash-image",
        provider: str = "auto",
        workflow_name: Optional[str] = None,
        **kwargs
    ) -> bytes:
        """
        Generate image with provider routing

        Args:
            prompt: Text prompt
            image_path: Reference image (required for Gemini/ComfyUI)
            model: Model name
            provider: "auto", "gemini", "dalle", "comfyui"
            workflow_name: ComfyUI workflow template (if provider=comfyui)
            **kwargs: Additional parameters (seed, steps, cfg, etc.)

        Returns:
            Image bytes
        """
        # Select provider
        provider = await self.select_provider(
            provider,
            model,
            require_reference_image=(image_path is not None)
        )

        logger.info(
            f"Generating image with provider: {provider}, model: {model}"
        )

        # Route to appropriate provider
        if provider == "comfyui":
            if not image_path:
                raise ValueError("image_path required for ComfyUI generation")

            # Use workflow_name or infer from model
            if not workflow_name:
                if "faceid" in model.lower():
                    workflow_name = "ip_adapter_faceid"
                elif "flux" in model.lower():
                    workflow_name = "flux_dev"
                else:
                    workflow_name = "sdxl_base"

            # Prepare parameters
            parameters = {
                "prompt": prompt,
                "image_path": str(image_path),
                "seed": kwargs.get("seed", 42),
                "steps": kwargs.get("steps", 20),
                "cfg": kwargs.get("guidance_scale", 7.5),
                "width": kwargs.get("width", 1024),
                "height": kwargs.get("height", 1024),
            }

            return await self.comfyui_service.generate_image(
                workflow_name,
                parameters
            )

        elif provider == "gemini":
            return await self.agenerate_image_with_gemini(
                prompt,
                image_path,
                model,
                **kwargs
            )

        elif provider == "dalle":
            if image_path:
                logger.warning(
                    "DALL-E does not support reference images, "
                    "image_path will be ignored"
                )
            return await self.agenerate_image_with_dalle(prompt, model, **kwargs)

        else:
            raise ValueError(f"Unknown provider: {provider}")
```

### Configuration Management

#### Extended models.yaml

**File**: `configs/models.yaml`

```yaml
# Existing Gemini/OpenAI configs
...

# ComfyUI Configuration
comfyui:
  enabled: true
  url: "http://192.168.1.100:8188"
  timeout: 300
  health_check_interval: 30

  # Available workflows
  workflows:
    ip_adapter_faceid:
      file: "ip_adapter_faceid.json"
      description: "IP-Adapter FaceID for identity preservation"
      models: ["sdxl"]
      vram_required: "12GB"
      avg_time_seconds: 22

    flux_dev:
      file: "flux_dev.json"
      description: "Flux.1 Dev high-quality generation"
      models: ["flux-dev"]
      vram_required: "16GB"
      avg_time_seconds: 30

    sdxl_controlnet_openpose:
      file: "sdxl_controlnet_openpose.json"
      description: "SDXL + ControlNet (pose preservation)"
      models: ["sdxl"]
      vram_required: "14GB"
      avg_time_seconds: 25

# Provider Priority (per tool)
provider_priority:
  fantasy_race_transformer:
    - provider: comfyui
      models: ["sdxl"]
      workflows: ["ip_adapter_faceid"]
      fallback_on_failure: true
    - provider: gemini
      models: ["gemini-2.5-flash-image"]

  modular_image_generator:
    - provider: comfyui
      models: ["sdxl", "flux-dev"]
      condition: "cost_sensitive"  # Prefer local for cost savings
    - provider: gemini
      models: ["gemini-2.5-flash-image"]

  style_transfer_generator:
    - provider: gemini  # Gemini-only (multimodal understanding)
      models: ["gemini-2.5-flash-image"]

# Default models per tool
defaults:
  fantasy_race_transformer: "sdxl"  # Will use ComfyUI if available
  modular_image_generator: "gemini-2.5-flash-image"
  style_transfer_generator: "gemini-2.5-flash-image"

  # ... existing tool configs ...
```

#### Environment Variables

**File**: `.env`

```bash
# Existing API keys
GEMINI_API_KEY=your_gemini_key
OPENAI_API_KEY=your_openai_key

# ComfyUI Configuration (NEW)
COMFYUI_ENABLED=true
COMFYUI_URL=http://192.168.1.100:8188
COMFYUI_TIMEOUT=300
COMFYUI_HEALTH_CHECK_INTERVAL=30

# Provider preferences (NEW)
DEFAULT_IMAGE_PROVIDER=auto  # auto, gemini, dalle, comfyui
PREFER_LOCAL_GENERATION=true  # Use ComfyUI when available
FALLBACK_TO_CLOUD=true  # Fall back to Gemini if ComfyUI fails

# Existing configs
REQUIRE_AUTH=true
JWT_SECRET_KEY=your_secret
...
```

### API Route Updates

#### Generator Routes Extension

**File**: `api/routes/generators.py`

Add provider selection to request models:

```python
from pydantic import BaseModel, Field
from typing import Optional

class ModularGenerateRequest(BaseModel):
    """Request for modular image generation"""
    subject_image: str

    # Provider selection (NEW)
    provider: str = Field(
        default="auto",
        description="Image generation provider (auto, gemini, dalle, comfyui)"
    )

    # Modular specs (existing)
    outfit: Optional[str] = None
    expression: Optional[str] = None
    visual_style: Optional[str] = None
    # ... other specs ...

class FantasyRaceTransformRequest(BaseModel):
    """Request for fantasy race transformation"""
    subject_image: str
    race: str = Field(..., description="Fantasy race (elf, orc, dwarf, tiefling, etc.)")
    subrace: Optional[str] = Field(None, description="Subrace variant (high-elf, drow, etc.)")
    intensity: float = Field(0.5, ge=0.0, le=1.0, description="Transformation intensity (0-1)")

    # Provider selection
    provider: str = Field(
        default="auto",
        description="Prefer ComfyUI for best identity preservation"
    )

    # Advanced options
    custom_features: Optional[Dict[str, str]] = None
    art_style: str = Field(default="dnd_official", description="Art style preset")

@router.post("/fantasy-race-transform")
async def transform_to_fantasy_race(
    request: FantasyRaceTransformRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: Optional[User] = Depends(get_current_active_user)
):
    """
    Transform photo into D&D fantasy race

    Uses IP-Adapter-FaceID on ComfyUI for maximum identity preservation
    Falls back to Gemini if ComfyUI unavailable
    """
    from api.services.generator_service import GeneratorService

    generator_service = GeneratorService(db)

    # Create job for tracking
    job_manager = get_job_manager()
    job_id = job_manager.create_job(
        job_type=JobType.IMAGE_GENERATION,
        title=f"Transform to {request.race}",
        total_steps=1
    )

    try:
        job_manager.start_job(job_id)

        # Generate transformation
        result = await generator_service.transform_to_fantasy_race(
            subject_image=request.subject_image,
            race=request.race,
            subrace=request.subrace,
            intensity=request.intensity,
            provider=request.provider,
            art_style=request.art_style,
            custom_features=request.custom_features
        )

        job_manager.complete_job(job_id, result)

        return {
            "status": "completed",
            "job_id": job_id,
            "result": result
        }

    except Exception as e:
        logger.error(f"Fantasy race transformation failed: {e}")
        job_manager.fail_job(job_id, str(e))

        return {
            "status": "failed",
            "job_id": job_id,
            "error": str(e)
        }
```

### Frontend Integration

#### Provider Selection UI

**Component**: `frontend/src/components/ProviderSelector.jsx`

```jsx
import React from 'react';

export function ProviderSelector({ value, onChange, disabled = false }) {
  return (
    <div className="provider-selector">
      <label htmlFor="provider">Image Provider</label>
      <select
        id="provider"
        value={value}
        onChange={(e) => onChange(e.target.value)}
        disabled={disabled}
      >
        <option value="auto">Auto (Best Available)</option>
        <option value="comfyui">Local (ComfyUI - Free, High Quality)</option>
        <option value="gemini">Gemini (Cloud, Fast)</option>
        <option value="dalle">DALL-E 3 (Cloud, Artistic)</option>
      </select>

      <div className="provider-info">
        {value === 'auto' && (
          <p>Automatically selects best provider based on availability and cost</p>
        )}
        {value === 'comfyui' && (
          <p>Uses local RTX 4090. Best quality, free, but requires network PC to be online.</p>
        )}
        {value === 'gemini' && (
          <p>Cloud API (~$0.04/image). Fast, supports reference images.</p>
        )}
        {value === 'dalle' && (
          <p>Cloud API (~$0.08/image). Artistic quality, no reference image support.</p>
        )}
      </div>
    </div>
  );
}
```

#### Fantasy Race Transformer UI

**Component**: `frontend/src/FantasyRaceTransformer.jsx`

```jsx
import React, { useState } from 'react';
import { api } from './services/api';
import { ProviderSelector } from './components/ProviderSelector';

const RACES = {
  elf: {
    name: 'Elf',
    subraces: ['High Elf', 'Wood Elf', 'Dark Elf (Drow)'],
    description: 'Pointed ears, elegant features, ethereal appearance'
  },
  orc: {
    name: 'Orc / Half-Orc',
    subraces: ['Full Orc', 'Half-Orc'],
    description: 'Tusks, green skin, muscular features'
  },
  dwarf: {
    name: 'Dwarf',
    subraces: ['Mountain Dwarf', 'Hill Dwarf', 'Gray Dwarf (Duergar)'],
    description: 'Stocky build, prominent beard, strong features'
  },
  tiefling: {
    name: 'Tiefling',
    subraces: ['Asmodeus', 'Zariel', 'Levistus'],
    description: 'Horns, tail, solid-color eyes, supernatural appearance'
  }
};

export default function FantasyRaceTransformer() {
  const [subjectImage, setSubjectImage] = useState('');
  const [race, setRace] = useState('elf');
  const [subrace, setSubrace] = useState('');
  const [intensity, setIntensity] = useState(0.5);
  const [provider, setProvider] = useState('auto');
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const handleTransform = async () => {
    setLoading(true);
    try {
      const response = await api.post('/generators/fantasy-race-transform', {
        subject_image: subjectImage,
        race,
        subrace,
        intensity,
        provider
      });
      setResult(response.data.result);
    } catch (error) {
      console.error('Transformation failed:', error);
      alert('Failed to transform image');
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="fantasy-race-transformer">
      <h1>Fantasy Race Transformer</h1>
      <p>Transform your photo into a D&D character while maintaining your recognizable features</p>

      <div className="form-section">
        <label>Subject Image</label>
        <input
          type="text"
          placeholder="Path to your photo"
          value={subjectImage}
          onChange={(e) => setSubjectImage(e.target.value)}
        />
      </div>

      <div className="form-section">
        <label>Race</label>
        <select value={race} onChange={(e) => {
          setRace(e.target.value);
          setSubrace('');
        }}>
          {Object.entries(RACES).map(([key, data]) => (
            <option key={key} value={key}>{data.name}</option>
          ))}
        </select>
        <p className="description">{RACES[race].description}</p>
      </div>

      {RACES[race].subraces.length > 0 && (
        <div className="form-section">
          <label>Subrace (Optional)</label>
          <select value={subrace} onChange={(e) => setSubrace(e.target.value)}>
            <option value="">-- Generic {RACES[race].name} --</option>
            {RACES[race].subraces.map(sr => (
              <option key={sr} value={sr}>{sr}</option>
            ))}
          </select>
        </div>
      )}

      <div className="form-section">
        <label>Transformation Intensity: {Math.round(intensity * 100)}%</label>
        <input
          type="range"
          min="0"
          max="1"
          step="0.05"
          value={intensity}
          onChange={(e) => setIntensity(parseFloat(e.target.value))}
        />
        <div className="intensity-labels">
          <span>Subtle (20%)</span>
          <span>Moderate (50%)</span>
          <span>Extreme (100%)</span>
        </div>
      </div>

      <ProviderSelector value={provider} onChange={setProvider} />

      <button
        onClick={handleTransform}
        disabled={loading || !subjectImage}
        className="transform-button"
      >
        {loading ? 'Transforming...' : 'Transform to ' + RACES[race].name}
      </button>

      {result && (
        <div className="result-section">
          <h2>Result</h2>
          <img src={`/output/${result.file_path}`} alt="Transformed character" />
          <div className="result-metadata">
            <p><strong>Provider:</strong> {result.metadata.provider}</p>
            <p><strong>Generation Time:</strong> {result.metadata.duration}s</p>
            {result.metadata.cost && (
              <p><strong>Cost:</strong> ${result.metadata.cost.toFixed(4)}</p>
            )}
          </div>
        </div>
      )}
    </div>
  );
}
```

---

## Implementation Roadmap

### Phase 1: ComfyUI Infrastructure Setup (Week 1)

**Goal**: Get ComfyUI running on RTX 4090 PC with API access

#### Tasks:
1. ‚úÖ **RTX 4090 PC Preparation**
   - Install OS (Windows 11 + WSL2 OR Ubuntu 24.04)
   - Install NVIDIA drivers (latest)
   - Install Docker + NVIDIA Container Toolkit
   - Configure static IP address
   - Test GPU access: `nvidia-smi`

2. ‚úÖ **ComfyUI Docker Deployment**
   - Create directory structure (`~/comfyui/{run,basedir,workflows,output}`)
   - Deploy docker-compose.yml
   - Start container: `docker-compose up -d`
   - Verify health: `curl http://localhost:8188/system_stats`

3. ‚úÖ **Model Downloads**
   - Download SDXL base (~7GB)
   - Download IP-Adapter-FaceID models (~1GB)
   - Download InsightFace (antelopev2) (~300MB)
   - Download CLIP vision encoder (~2GB)
   - Total: ~15-20GB initial download

4. ‚úÖ **Custom Nodes Installation**
   - Install ComfyUI_IPAdapter_plus
   - Install ComfyUI_essentials
   - Install comfyui_controlnet_aux
   - Restart container
   - Verify nodes available: GET /object_info

5. ‚úÖ **Network Configuration**
   - Expose port 8188 to network
   - Test from Mac Studio: `curl http://192.168.1.100:8188/system_stats`
   - Configure firewall (if needed)
   - Document IP address for Life-OS integration

**Deliverables**:
- ComfyUI running on RTX 4090 PC
- Accessible via network from Mac Studio
- Essential models downloaded
- Health check endpoint responsive

**Testing**:
```bash
# From Mac Studio
curl http://192.168.1.100:8188/system_stats | jq

# Expected output:
{
  "system": {
    "os": "Linux",
    "python_version": "3.11.x",
    "vram_total": 24564,
    "vram_used": 2345
  },
  "devices": [...],
  "models": [...]
}
```

---

### Phase 2: Workflow Development (Week 2)

**Goal**: Create and test ComfyUI workflow templates for fantasy race transformation

#### Tasks:
1. ‚úÖ **IP-Adapter-FaceID Workflow**
   - Open ComfyUI web interface (http://192.168.1.100:8188)
   - Build workflow:
     - Load Checkpoint (SDXL)
     - Load IP-Adapter-FaceID model
     - Load reference image (subject photo)
     - CLIP Text Encode (positive: race description)
     - CLIP Text Encode (negative: quality issues)
     - IP-Adapter Apply (with FaceID)
     - KSampler (generate)
     - VAE Decode
     - Save Image
   - Test with sample photo
   - Export as "Save (API Format)" ‚Üí `ip_adapter_faceid.json`
   - Store in Life-OS: `api/workflows/comfyui/ip_adapter_faceid.json`

2. ‚úÖ **Workflow Parameter Mapping**
   - Document node IDs for each parameter:
     ```
     Node 3: KSampler ‚Üí seed, steps, cfg, sampler_name
     Node 6: CLIP Positive ‚Üí text (prompt)
     Node 7: CLIP Negative ‚Üí text (negative prompt)
     Node 5: Empty Latent ‚Üí width, height
     ```
   - Create parameter mapping documentation
   - Test parameter substitution manually

3. ‚úÖ **Alternative Workflows**
   - Create `flux_dev.json` (Flux.1 Dev model)
   - Create `sdxl_controlnet_openpose.json` (pose preservation)
   - Create `sdxl_base.json` (simple SDXL without IP-Adapter)
   - Export all as API format
   - Test each workflow

4. ‚úÖ **Workflow Validation**
   - For each workflow:
     - Test with different prompts
     - Verify output quality
     - Measure generation time
     - Document VRAM usage
     - Identify optimal parameters (steps, cfg, etc.)

**Deliverables**:
- 4+ workflow templates in API format
- Parameter mapping documentation
- Performance benchmarks per workflow
- Known issues/limitations documented

**Testing Checklist**:
- [ ] IP-Adapter-FaceID preserves face structure
- [ ] Elf transformation adds pointed ears correctly
- [ ] Orc transformation adds tusks and green skin
- [ ] Skin tone shifts maintain undertones
- [ ] Generated images are 1024x1024
- [ ] Average generation time < 30 seconds

---

### Phase 3: Life-OS Integration (Week 2-3)

**Goal**: Integrate ComfyUI as a provider in Life-OS

#### Tasks:
1. ‚úÖ **ComfyUIService Implementation**
   - Create `api/services/comfyui_service.py`
   - Implement:
     - `is_healthy()` - health check with caching
     - `load_workflow_template()` - load JSON from file
     - `apply_parameters()` - parameter substitution
     - `queue_prompt()` - submit job to ComfyUI
     - `wait_for_completion()` - poll for results
     - `get_output_image()` - download generated image
     - `generate_image()` - high-level wrapper
   - Add error handling and logging
   - Write unit tests

2. ‚úÖ **LLMRouter Enhancement**
   - Add ComfyUI provider to `ai_tools/shared/router.py`
   - Implement `select_provider()` auto-routing logic
   - Extend `agenerate_image()` to support provider parameter
   - Add fallback mechanism (ComfyUI ‚Üí Gemini)
   - Update docstrings and type hints

3. ‚úÖ **Configuration Management**
   - Update `configs/models.yaml`:
     - Add `comfyui` section
     - Define workflow mappings
     - Set provider priorities per tool
   - Add environment variables to `.env`:
     - `COMFYUI_ENABLED=true`
     - `COMFYUI_URL=http://192.168.1.100:8188`
   - Update `api/config.py` to load ComfyUI settings

4. ‚úÖ **Dependency Installation**
   - Add to `api/requirements.txt`:
     ```
     comfyui-workflow-client==0.1.5
     ```
   - Rebuild API container: `docker-compose up -d --build api`
   - Verify imports work

5. ‚úÖ **Testing Integration**
   - Create `tests/test_comfyui_service.py`
   - Test cases:
     - Health check (ComfyUI online/offline)
     - Workflow loading
     - Parameter substitution
     - End-to-end generation
     - Timeout handling
     - Error recovery
   - Run tests: `docker exec ai-studio-api pytest tests/test_comfyui_service.py -v`

**Deliverables**:
- ComfyUIService fully functional
- LLMRouter supports "comfyui" provider
- Configuration files updated
- Unit tests passing
- Integration tests passing

**Testing Commands**:
```bash
# Health check test
docker exec ai-studio-api python3 -c "
import asyncio
from api.services.comfyui_service import ComfyUIService

async def test():
    service = ComfyUIService()
    print('Healthy:', await service.is_healthy())

asyncio.run(test())
"

# Generation test
docker exec ai-studio-api python3 -c "
import asyncio
from api.services.comfyui_service import ComfyUIService

async def test():
    service = ComfyUIService()
    params = {
        'prompt': 'portrait of a person as a high elf',
        'seed': 12345,
        'steps': 20
    }
    img_bytes = await service.generate_image('ip_adapter_faceid', params)
    print(f'Generated {len(img_bytes)} bytes')

    with open('/app/output/test_comfyui.png', 'wb') as f:
        f.write(img_bytes)
    print('Saved to output/test_comfyui.png')

asyncio.run(test())
"
```

---

### Phase 4: Fantasy Race Transformer Tool (Week 3-4)

**Goal**: Build fantasy race transformation tool with race templates

#### Tasks:
1. ‚úÖ **Race Template System**
   - Create `api/models/race_templates.py`:
     ```python
     from pydantic import BaseModel
     from typing import Dict, List

     class RaceTemplate(BaseModel):
         race_name: str
         race_category: str
         base_intensity: float
         structural_changes: Dict[str, str]
         additive_features: Dict[str, Any]
         artistic_direction: Dict[str, str]
         forbidden_changes: List[str]
     ```
   - Create templates for:
     - Elf (+ High Elf, Wood Elf, Drow)
     - Orc / Half-Orc
     - Dwarf (+ Mountain, Hill, Duergar)
     - Tiefling (+ Asmodeus, Zariel, Levistus)
   - Store in `api/data/race_templates/*.json`

2. ‚úÖ **FantasyRaceTransformer Tool**
   - Create `ai_tools/fantasy_race_transformer/tool.py`
   - Inherit from ModularImageGenerator
   - Implement:
     - `transform_to_race()` - main transformation method
     - `_load_race_template()` - load race definition
     - `_build_race_prompt()` - construct prompt from template
     - `_apply_intensity()` - scale transformation strength
   - Add template.md and README.md
   - Register in tool registry

3. ‚úÖ **API Routes**
   - Create `/api/generators/fantasy-race-transform` endpoint
   - Request model: `FantasyRaceTransformRequest`
   - Response model: `ImageGenerationResult`
   - Job queue integration for long-running tasks
   - Error handling with fallback

4. ‚úÖ **GeneratorService Extension**
   - Add `transform_to_fantasy_race()` method to `api/services/generator_service.py`
   - Handle provider selection logic
   - Implement retry with fallback (ComfyUI ‚Üí Gemini)
   - Track cost and generation time

**Deliverables**:
- Race template system with 4+ races
- FantasyRaceTransformer tool functional
- API endpoint working
- Integration with job queue
- Error handling and fallback

**Testing**:
```bash
# Test race transformation
curl -X POST http://localhost:8000/api/generators/fantasy-race-transform \
  -H "Content-Type: application/json" \
  -d '{
    "subject_image": "subjects/test_portrait.jpg",
    "race": "elf",
    "subrace": "high-elf",
    "intensity": 0.6,
    "provider": "comfyui"
  }'

# Expected response:
{
  "status": "completed",
  "job_id": "abc-123",
  "result": {
    "file_path": "output/generated/elf_transformation_001.png",
    "metadata": {
      "provider": "comfyui",
      "model": "sdxl",
      "workflow": "ip_adapter_faceid",
      "duration": 22.3,
      "cost": 0.0
    }
  }
}
```

---

### Phase 5: Frontend Development (Week 4)

**Goal**: Build UI for fantasy race transformation

#### Tasks:
1. ‚úÖ **ProviderSelector Component**
   - Create `frontend/src/components/ProviderSelector.jsx`
   - Dropdown with options: auto, comfyui, gemini, dalle
   - Display provider info and cost
   - Show ComfyUI health status indicator

2. ‚úÖ **FantasyRaceTransformer Page**
   - Create `frontend/src/FantasyRaceTransformer.jsx`
   - UI elements:
     - Subject image upload/selection
     - Race dropdown (elf, orc, dwarf, tiefling)
     - Subrace dropdown (dynamic based on race)
     - Intensity slider (0-100%)
     - Provider selector
     - Art style selector
     - Transform button
   - Result display:
     - Generated image
     - Metadata (provider, time, cost)
     - Download button
     - Save to favorites

3. ‚úÖ **Route Registration**
   - Add route to `frontend/src/App.jsx`:
     ```jsx
     <Route path="/fantasy-race-transformer" element={<FantasyRaceTransformer />} />
     ```
   - Add sidebar link

4. ‚úÖ **Styling**
   - Create `frontend/src/FantasyRaceTransformer.css`
   - Mobile-responsive design
   - Loading states
   - Error states
   - Result animations

5. ‚úÖ **Testing & Refinement**
   - Test on desktop (Chrome, Firefox, Safari)
   - Test on mobile (iOS, Android)
   - Test with different image sizes
   - Test error scenarios (ComfyUI offline, invalid image, etc.)
   - Performance optimization

**Deliverables**:
- FantasyRaceTransformer UI complete
- Mobile-responsive
- Provider selection working
- Real-time progress updates
- Error handling with user-friendly messages

**UI Mockup**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fantasy Race Transformer                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ Subject Image:  [Choose File] test_portrait.jpg‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Race:          [‚ñº Elf                        ]  ‚îÇ
‚îÇ Subrace:       [‚ñº High Elf (Optional)        ]  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Transformation Intensity:                       ‚îÇ
‚îÇ [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 60%                    ‚îÇ
‚îÇ  Subtle          Moderate          Extreme      ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Provider:      [‚ñº Auto (Best Available)      ]  ‚îÇ
‚îÇ ‚ÑπÔ∏è Will use ComfyUI for best quality           ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Art Style:     [‚ñº D&D Official Art           ]  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ        [Transform to High Elf]                  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Result:                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ   ‚îÇ
‚îÇ ‚îÇ         [Generated Image Preview]         ‚îÇ   ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ Provider: ComfyUI (Local)                       ‚îÇ
‚îÇ Generation Time: 22.3s                          ‚îÇ
‚îÇ Cost: $0.00                                     ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ [Download]  [Save to Favorites]  [Regenerate]  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Phase 6: Advanced Features (Week 5-6)

**Goal**: Add advanced capabilities and optimizations

#### Tasks:
1. ‚úÖ **Multi-Shot Reference Support**
   - Upload multiple photos of same person
   - Face feature extraction and averaging
   - Enhanced identity preservation
   - UI: drag-and-drop multiple images

2. ‚úÖ **Batch Transformation**
   - Transform entire D&D party at once
   - Upload multiple subject images
   - Select race for each person
   - Generate "party portrait" composite

3. ‚úÖ **Transformation Presets**
   - Save favorite race configurations
   - Preset library (community presets)
   - One-click apply preset
   - Share presets with other users

4. ‚úÖ **Advanced Race Features**
   - Age progression (young elf ‚Üí ancient elf)
   - Gender presentation slider
   - Custom feature emphasis (larger eyes, broader shoulders, etc.)
   - Scars/tattoos/accessories

5. ‚úÖ **ControlNet Integration**
   - Pose preservation (match reference pose exactly)
   - Depth-guided generation
   - Canny edge detection
   - Openpose skeleton matching

6. ‚úÖ **Performance Optimizations**
   - Result caching (same subject + race ‚Üí cache hit)
   - Progressive image loading
   - Thumbnail generation
   - Background job queue prioritization

**Deliverables**:
- Multi-shot reference working
- Batch transformation functional
- Preset system implemented
- ControlNet workflows integrated
- Performance benchmarks improved

**Testing**:
- [ ] Multi-shot improves identity preservation
- [ ] Batch generates 5 characters in < 2 minutes
- [ ] Presets apply correctly
- [ ] ControlNet preserves pose accurately
- [ ] Cache reduces regeneration time by 90%

---

## Cost & Performance Analysis

### Cost Comparison (1000 Images/Month)

| Provider | Cost per Image | Monthly Cost | Notes |
|----------|---------------|--------------|-------|
| **ComfyUI (Local)** | ~$0.001 | **~$1-5** | Electricity only (~$0.10/hr, 450W) |
| **Gemini 2.5 Flash** | ~$0.04 | **$40** | Cloud API, fast, multimodal |
| **DALL-E 3** | ~$0.08 | **$80** | Cloud API, artistic, no reference image |

**Break-Even Point**:
- RTX 4090 hardware cost: ~$1,600
- Break-even vs. Gemini: **40 images** ($1,600 √∑ $40 savings/month ‚âà 40 months... but at scale)
- Actually: **400 images total** ($1,600 √∑ $0.04 = 40,000... wait, $1,600 / ($0.04 - $0.001) ‚âà 41,000 images)

**Corrected Break-Even**:
- Savings per image: $0.04 (Gemini) - $0.001 (local) = $0.039
- Total images to break even: $1,600 √∑ $0.039 = **41,026 images**
- At 1000 images/month: **41 months** (~3.4 years)

**BUT**: ComfyUI unlocks features not available at any price via cloud APIs (IP-Adapter-FaceID, ControlNet, custom LoRAs), so ROI includes quality improvements, not just cost.

### Performance Benchmarks

#### Generation Speed

| Model/Workflow | RTX 4090 | Gemini 2.5 | DALL-E 3 |
|----------------|----------|------------|----------|
| **SDXL Base** | 15-20s | 30-60s | 15-30s |
| **IP-Adapter-FaceID** | 18-25s | N/A | N/A |
| **Flux.1 Dev** | 25-35s | N/A | N/A |
| **ControlNet + SDXL** | 20-28s | N/A | N/A |

**Key Takeaway**: Local generation is often **faster** than cloud for SDXL, especially for advanced workflows.

#### Identity Preservation Quality

| Technique | Face Similarity Score | Method |
|-----------|----------------------|---------|
| **IP-Adapter-FaceID-PlusV2** | **0.85-0.92** | Local (ComfyUI) |
| **Gemini with Reference Image** | 0.70-0.82 | Cloud API |
| **DALL-E 3 (Text-Only)** | 0.30-0.50 | Cloud API |

**Face Similarity Score**: 0.0 (completely different) to 1.0 (identical)
**Source**: IP-Adapter-FaceID research paper (2024)

#### VRAM Usage

| Workflow | VRAM Required | RTX 4090 Headroom |
|----------|---------------|-------------------|
| SDXL Base | 8-10 GB | ‚úÖ 14-16 GB free |
| IP-Adapter-FaceID + SDXL | 12-14 GB | ‚úÖ 10-12 GB free |
| Flux.1 Dev | 16-18 GB | ‚úÖ 6-8 GB free |
| ControlNet + SDXL | 10-12 GB | ‚úÖ 12-14 GB free |
| **Batch (4 images)** | 18-20 GB | ‚úÖ 4-6 GB free |

**Conclusion**: RTX 4090's 24GB is sufficient for all workflows, including batch processing.

---

## Integration with Existing Life-OS Features

### Composability with Existing Tools

The fantasy race transformer integrates seamlessly with existing Life-OS features:

#### 1. **Modular Image Generator**
Fantasy race becomes another modular component:
```
Subject Image +
Fantasy Race (elf) +
Outfit (leather armor) +
Expression (confident warrior) +
Visual Style (D&D official art) +
Makeup (battle scars)
= Complete D&D Character Portrait
```

**Implementation**: Extend `ModularImageGenerator` to accept `fantasy_race` parameter:
```python
result = modular_generator.generate(
    subject_image="subjects/player_photo.jpg",
    fantasy_race="high-elf",
    outfit="leather_armor_preset",
    expression="confident_warrior",
    visual_style="dnd_official_art"
)
```

#### 2. **Character Entity System**
Characters can store their fantasy race transformation:
```python
# Database schema addition
class Character(Base):
    # ... existing fields ...

    fantasy_race: Mapped[Optional[str]]  # "high-elf", "half-orc", etc.
    race_transformation_image_id: Mapped[Optional[str]]  # FK to images table
    transformation_intensity: Mapped[Optional[float]]  # 0.0-1.0
```

**Workflow**:
1. User creates character from photo
2. Transforms to fantasy race
3. Character entity stores both original and transformed images
4. Future generations use transformed image as subject

#### 3. **Story Illustration Workflow**
Characters can be illustrated as their fantasy race versions:
```
Story Scene: "The elf archer draws her bow"
‚Üí Uses character's race transformation image as subject
‚Üí Generates scene with consistent character appearance
```

#### 4. **Favorites System**
Save favorite transformations:
- "My Half-Orc Barbarian"
- "My High Elf Wizard"
- "My Tiefling Warlock"

#### 5. **Preset System**
Race transformation presets can be saved and shared:
```json
{
  "preset_id": "epic_high_elf",
  "race": "elf",
  "subrace": "high-elf",
  "intensity": 0.65,
  "custom_features": {
    "ear_length": "extra_long",
    "skin_tone": "pale_luminous",
    "eye_enhancement": "mystical_glow"
  },
  "art_style": "dnd_official_art",
  "tags": ["elf", "heroic", "aristocratic"]
}
```

---

## Future Expansion Possibilities

### Beyond D&D: Other Transformation Types

The same infrastructure enables many transformation categories:

#### 1. **Supernatural Creatures**
- **Vampires**: Pale skin, fangs, red eyes, aristocratic features
- **Werewolves**: Partial transformation, facial hair, animalistic eyes
- **Zombies**: Decay effects, pale/green skin, hollow eyes
- **Ghosts**: Ethereal, translucent, glowing

#### 2. **Sci-Fi Races**
- **Star Trek**: Vulcans (pointed ears, arched eyebrows), Klingons (forehead ridges), Andorians (blue skin, antennae)
- **Star Wars**: Twi'leks (head-tails), Togruta (montrals), Chiss (blue skin, red eyes)
- **Custom Aliens**: User-defined features

#### 3. **Age Progression**
- **Youth**: De-age by 10-30 years
- **Aging**: Add 10-50 years with wrinkles, gray hair
- **Life Stages**: Baby ‚Üí Child ‚Üí Teen ‚Üí Adult ‚Üí Elderly

#### 4. **Gender Presentation**
- Explore different gender presentations
- Maintain identity while shifting features
- Respectful, user-controlled transformations

#### 5. **Animal Hybrids**
- **Cat Person (Tabaxi)**: Feline features, whiskers, ears
- **Bird Person (Aarakocra)**: Feathers, beak, avian eyes
- **Lizard Person (Dragonborn)**: Scales, reptilian features
- **Custom Hybrids**: User-defined animal + human blend

#### 6. **Fantasy Creatures (Extreme Transformations)**
- **Merfolk**: Upper body human, lower body fish
- **Centaur**: Upper body human, lower body horse
- **Dragonborn**: Fully draconic head with humanoid eyes
- **Elemental Touched**: Fire/water/earth/air visual effects

### Video Generation (Phase 7+)

With local infrastructure in place, extend to video:

#### 1. **Character Turnaround**
- Generate 360¬∞ rotation of character
- Consistent appearance from all angles
- Useful for 3D modeling references

#### 2. **Animated Portraits**
- Subtle breathing animation
- Eye blinks and micro-expressions
- "Living painting" effect

#### 3. **Action Sequences**
- Character performing combat moves
- Spellcasting animations
- Emotive reactions

**Implementation**: Add AnimateDiff or similar to ComfyUI workflows

### Custom Model Fine-Tuning

#### 1. **Personal LoRAs**
- Train LoRA on user's photos (10-20 images)
- Perfect identity preservation across all generations
- One-time training, reusable forever

#### 2. **Art Style LoRAs**
- Fine-tune on specific D&D artists' styles
- Community-shared style LoRAs
- Custom campaign aesthetic

#### 3. **Race-Specific LoRAs**
- Specialized elf feature LoRA
- Orc/tusk enhancement LoRA
- Tiefling horn variety LoRA

**Training**: Use Kohya_ss scripts, ~30 min training on RTX 4090

---

## Risk Mitigation & Considerations

### Technical Risks

#### 1. **ComfyUI Unavailability**
**Risk**: Network PC offline, GPU failure, workflow breaks

**Mitigation**:
- Auto-fallback to Gemini (implemented in provider selection)
- Health check with retry logic
- User notification when ComfyUI offline
- Option to manually select provider

#### 2. **Model Download Failures**
**Risk**: Hugging Face unavailable, corrupted downloads

**Mitigation**:
- Pre-download essential models
- Local model mirror/cache
- Checksums for verification
- Manual upload option

#### 3. **VRAM Overflow**
**Risk**: Complex workflows exceed 24GB VRAM

**Mitigation**:
- Monitor VRAM usage via `/system_stats` endpoint
- Reject jobs if insufficient VRAM
- Queue management (one job at a time for heavy workflows)
- Model unloading between jobs

#### 4. **Network Latency**
**Risk**: Slow network between Mac Studio and RTX PC

**Mitigation**:
- Use gigabit ethernet (not WiFi)
- Image compression for upload
- Progress streaming via WebSocket
- Timeout warnings to user

### Ethical Considerations

#### 1. **Identity & Representation**
**Concern**: Transformations should be respectful, user-controlled

**Guidelines**:
- User explicitly requests transformation
- Clear preview before finalizing
- Option to adjust intensity
- No surprise changes

#### 2. **Cultural Sensitivity**
**Concern**: Fantasy races can have problematic real-world parallels

**Guidelines**:
- Focus on D&D official lore
- Avoid reinforcing stereotypes
- User-defined custom features
- Educational context (D&D character creation)

#### 3. **Data Privacy**
**Concern**: User photos processed locally

**Guidelines**:
- Photos never leave user's network (Mac ‚Üî RTX PC)
- No cloud upload unless user chooses
- Optional: auto-delete source images after generation
- Transparent data handling

### Legal Considerations

#### 1. **Model Licenses**
**Models Used**:
- SDXL: CreativeML Open RAIL++-M License (commercial allowed)
- IP-Adapter: Apache 2.0 (commercial allowed)
- InsightFace: MIT License (commercial allowed)
- Flux.1 Dev: Non-commercial (check license for commercial use)

**Compliance**:
- Document all model licenses
- Restrict Flux.1 Dev to non-commercial use OR upgrade to Flux.1 Pro
- Provide attribution where required

#### 2. **Generated Image Ownership**
**Principle**: User owns their transformed images

**Implementation**:
- Clear terms of service
- User retains all rights
- Platform doesn't claim ownership
- Optional watermark for attribution

---

## Success Metrics

### Phase 1-3 Success Criteria (Infrastructure)

**Technical Metrics**:
- [ ] ComfyUI health check success rate > 99%
- [ ] Average generation time < 30 seconds
- [ ] API response time < 500ms (excluding generation)
- [ ] Zero data loss (all generated images saved)
- [ ] Fallback to Gemini success rate > 95%

**Quality Metrics**:
- [ ] Face similarity score > 0.80 (IP-Adapter-FaceID)
- [ ] User satisfaction score > 4.0/5.0
- [ ] Recognizability test: 80% of testers identify person

### Phase 4-5 Success Criteria (Feature Complete)

**Functional Metrics**:
- [ ] All 4 core races supported (elf, orc, dwarf, tiefling)
- [ ] 12+ subrace variations available
- [ ] Intensity slider functional (0-100%)
- [ ] Provider selection working (auto, comfyui, gemini)
- [ ] Frontend mobile-responsive

**Usage Metrics**:
- [ ] 50+ transformations generated (beta testing)
- [ ] Average user generates 3+ races per character
- [ ] 70% choose ComfyUI when available
- [ ] Error rate < 5%

### Phase 6 Success Criteria (Advanced Features)

**Advanced Metrics**:
- [ ] Multi-shot improves similarity by 10%+
- [ ] Batch processing: 5 characters < 2 minutes
- [ ] Preset library: 20+ community presets
- [ ] Cache hit rate > 40% (repeat generations)

**Business Metrics**:
- [ ] Cost reduction: 90% vs. all-cloud approach
- [ ] Generation capacity: 1000+ images/month sustainable
- [ ] Infrastructure uptime > 99.5%

---

## Documentation Requirements

### User Documentation

1. **Getting Started Guide**
   - How to transform your photo into a D&D character
   - Choosing the right race
   - Understanding intensity slider
   - Provider selection tips

2. **Race Compendium**
   - Visual guide to each race
   - Subrace differences
   - Example transformations
   - Customization options

3. **Troubleshooting**
   - ComfyUI offline ‚Üí what to do
   - Image quality issues
   - Identity preservation problems
   - Provider comparison

### Developer Documentation

1. **ComfyUI Setup Guide**
   - Hardware requirements
   - Docker deployment
   - Model downloads
   - Network configuration
   - Troubleshooting

2. **Workflow Development**
   - Creating new workflows
   - Exporting API format
   - Parameter mapping
   - Testing workflows

3. **Integration Guide**
   - Adding new providers
   - Extending LLMRouter
   - Creating custom tools
   - Race template format

4. **API Reference**
   - `/fantasy-race-transform` endpoint
   - Request/response schemas
   - Error codes
   - Rate limiting

---

## Timeline Summary

| Phase | Duration | Key Deliverables |
|-------|----------|-----------------|
| **Phase 1: Infrastructure** | Week 1 | ComfyUI running on RTX 4090 |
| **Phase 2: Workflows** | Week 2 | 4+ workflow templates tested |
| **Phase 3: Integration** | Week 2-3 | ComfyUIService + LLMRouter |
| **Phase 4: Race Transformer** | Week 3-4 | 4 races with subraces |
| **Phase 5: Frontend** | Week 4 | UI complete, mobile-responsive |
| **Phase 6: Advanced** | Week 5-6 | Multi-shot, batch, presets |
| **TOTAL** | **6 weeks** | Production-ready fantasy race transformation |

---

## Next Steps

### Immediate Actions (This Week)

1. **Order/Confirm RTX 4090 PC Setup**
   - Verify hardware specs
   - Install OS (Windows 11 + WSL2 OR Ubuntu)
   - Install NVIDIA drivers
   - Configure network (static IP)

2. **Deploy ComfyUI Docker**
   - Follow Phase 1 setup instructions
   - Download essential models (~20GB)
   - Test health endpoint from Mac Studio

3. **Create Initial Workflow**
   - Build IP-Adapter-FaceID workflow in ComfyUI UI
   - Export as API format
   - Test with sample image

### Week 2 Actions

1. **Implement ComfyUIService**
   - Create service file
   - Write unit tests
   - Integrate with LLMRouter

2. **Create Race Templates**
   - Define elf, orc, dwarf, tiefling templates
   - Document transformation parameters

3. **Build Fantasy Race Transformer Tool**
   - Create tool structure
   - Implement prompt building
   - Test with ComfyUI

### Week 3-4 Actions

1. **API Routes & Frontend**
   - Add `/fantasy-race-transform` endpoint
   - Build React UI
   - Integrate provider selector

2. **Testing & Refinement**
   - Beta test with real users
   - Gather feedback on identity preservation
   - Refine prompts and parameters

3. **Documentation**
   - User guide
   - Developer setup guide
   - API documentation

---

## Appendix: Technical Reference

### ComfyUI Workflow JSON Structure

**Example: IP-Adapter-FaceID Workflow**

```json
{
  "1": {
    "inputs": {
      "image": "subject_photo.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {"title": "Load Subject Image"}
  },
  "2": {
    "inputs": {
      "ipadapter_file": "ip-adapter-faceid-plusv2_sdxl.bin"
    },
    "class_type": "IPAdapterModelLoader",
    "_meta": {"title": "Load IP-Adapter Model"}
  },
  "3": {
    "inputs": {
      "provider": "CPU"
    },
    "class_type": "InsightFaceLoader",
    "_meta": {"title": "Load InsightFace"}
  },
  "4": {
    "inputs": {
      "ckpt_name": "sd_xl_base_1.0.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {"title": "Load SDXL Checkpoint"}
  },
  "5": {
    "inputs": {
      "weight": 0.8,
      "weight_faceidv2": 0.8,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0.0,
      "end_at": 1.0,
      "embeds_scaling": "V only",
      "model": ["4", 0],
      "ipadapter": ["2", 0],
      "image": ["1", 0],
      "insightface": ["3", 0]
    },
    "class_type": "IPAdapterFaceID",
    "_meta": {"title": "Apply IP-Adapter FaceID"}
  },
  "6": {
    "inputs": {
      "text": "portrait of a person as a high elf with pointed ears, elegant features, ethereal appearance, detailed fantasy art, D&D character art",
      "clip": ["4", 1]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {"title": "CLIP Positive Prompt"}
  },
  "7": {
    "inputs": {
      "text": "low quality, blurry, deformed, ugly, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers",
      "clip": ["4", 1]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {"title": "CLIP Negative Prompt"}
  },
  "8": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {"title": "Empty Latent Image"}
  },
  "9": {
    "inputs": {
      "seed": 42,
      "steps": 20,
      "cfg": 7.0,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1.0,
      "model": ["5", 0],
      "positive": ["6", 0],
      "negative": ["7", 0],
      "latent_image": ["8", 0]
    },
    "class_type": "KSampler",
    "_meta": {"title": "KSampler"}
  },
  "10": {
    "inputs": {
      "samples": ["9", 0],
      "vae": ["4", 2]
    },
    "class_type": "VAEDecode",
    "_meta": {"title": "VAE Decode"}
  },
  "11": {
    "inputs": {
      "filename_prefix": "fantasy_race_transform",
      "images": ["10", 0]
    },
    "class_type": "SaveImage",
    "_meta": {"title": "Save Image"}
  }
}
```

**Node Connections**:
- `["4", 0]` means "output 0 from node 4"
- Node 4 outputs: [model, clip, vae]
- Node 5 takes model from node 4, IP-Adapter from node 2, image from node 1

---

### Environment Variables Reference

**Complete `.env` for ComfyUI Integration**:

```bash
# Life-OS Core
REQUIRE_AUTH=true
JWT_SECRET_KEY=your_secret_key_here
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=43200

# Database
DATABASE_URL=postgresql+asyncpg://lifeos:password@ai-studio-postgres:5432/lifeos

# Redis (Job Queue)
REDIS_URL=redis://ai-studio-redis:6379/0
USE_REDIS=true

# AI Provider Keys
GEMINI_API_KEY=your_gemini_key_here
OPENAI_API_KEY=your_openai_key_here

# ComfyUI Configuration (NEW)
COMFYUI_ENABLED=true
COMFYUI_URL=http://192.168.1.100:8188
COMFYUI_TIMEOUT=300
COMFYUI_HEALTH_CHECK_INTERVAL=30
COMFYUI_RETRY_ATTEMPTS=3
COMFYUI_RETRY_DELAY=5

# Provider Preferences (NEW)
DEFAULT_IMAGE_PROVIDER=auto
PREFER_LOCAL_GENERATION=true
FALLBACK_TO_CLOUD=true
COST_SENSITIVE_MODE=true

# Logging
LOG_LEVEL=INFO
STRUCTURED_LOGGING=true
```

---

## Conclusion

This initiative combines **fantasy race transformation** with **local image generation infrastructure** to create a powerful, cost-effective, and high-quality character creation system for D&D players and beyond.

**Key Benefits**:
1. **Identity Preservation**: IP-Adapter-FaceID achieves 85-92% face similarity (vs. 70-82% with cloud APIs)
2. **Cost Savings**: $0 per image after hardware investment (vs. $0.04-$0.08 cloud)
3. **Quality**: Access to advanced techniques not available via APIs
4. **Foundation**: Infrastructure enables future features (video, custom models, etc.)
5. **User Value**: Players get personalized D&D character art in seconds

**Next Step**: Begin Phase 1 (ComfyUI Infrastructure Setup) this week.
