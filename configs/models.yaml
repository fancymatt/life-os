# AI-Studio Model Configuration
#
# This file defines which models are used for each tool by default.
# Models can be overridden per-run using command-line flags.

# Default models for each tool
defaults:
  # Image Analysis Tools
  outfit_analyzer: "gemini/gemini-2.0-flash-exp"
  visual_style_analyzer: "gemini/gemini-2.0-flash-exp"
  art_style_analyzer: "gemini/gemini-2.0-flash-exp"
  hair_style_analyzer: "gemini/gemini-2.0-flash-exp"
  hair_color_analyzer: "gemini/gemini-2.0-flash-exp"
  makeup_analyzer: "gemini/gemini-2.0-flash-exp"
  expression_analyzer: "gemini/gemini-2.0-flash-exp"
  accessories_analyzer: "gemini/gemini-2.0-flash-exp"
  comprehensive_analyzer: "gemini/gemini-2.0-flash-exp"
  style_guide_analyzer: "gemini/gemini-2.0-flash-exp"
  character_appearance_analyzer: "ollama/gpt-oss:120b"  # Local 120B model (FREE!)

  # Image Generation Tools (using Gemini 2.5 Flash Image - dedicated image generation model)
  outfit_generator: "gemini-2.5-flash-image"
  style_transfer_generator: "gemini-2.5-flash-image"
  modular_image_generator: "gemini-2.5-flash-image"
  art_style_generator: "gemini-2.5-flash-image"
  style_guide_generator: "gemini-2.5-flash-image"
  combined_transformation: "gemini-2.5-flash-image"

  # Video Tools
  video_prompt_enhancer: "gpt-4o"
  sora_video_generator: "sora-2-pro"

  # General defaults
  timeout: 180
  retries: 3
  temperature: 0.7

# Environment-specific overrides
overrides:
  development:
    # Use faster/cheaper models in development
    outfit_analyzer: "gemini-1.5-flash"
    visual_style_analyzer: "gemini-1.5-flash"
    art_style_analyzer: "gemini-1.5-flash"
    video_prompt_enhancer: "gpt-4o-mini"
    sora_video_generator: "sora-2"

  production:
    # Use best models in production
    video_prompt_enhancer: "gpt-5"
    sora_video_generator: "sora-2-pro"

  experimental:
    # Try experimental models
    outfit_analyzer: "gemini-2.0-flash-exp"
    video_prompt_enhancer: "gpt-5"

# LiteLLM routing configuration
routing:
  # Timeout for API calls (seconds)
  timeout: 180

  # Number of retries on failure
  retries: 3

  # Fallback models if primary fails
  fallback_models:
    - "gemini/gemini-2.0-flash-exp"
    - "claude-3-5-sonnet"
    - "gpt-4o"

  # Rate limiting (requests per second)
  rate_limit:
    gemini: 2.0
    openai: 2.0
    anthropic: 2.0

# Model aliases (for convenience)
aliases:
  gemini: "gemini/gemini-2.0-flash-exp"
  gpt: "gpt-4o"
  claude: "claude-3-5-sonnet"
  sora: "sora-2-pro"
  # Local models via Ollama (prefix with "ollama/")
  llama: "ollama/llama3.2:3b"
  mistral: "ollama/mistral:7b"
  qwen: "ollama/qwen2.5:7b"
  codellama: "ollama/codellama:7b"

# Cost tracking (approximate USD per 1M tokens)
cost_estimates:
  input:
    gemini-2.0-flash: 0.15
    gemini-1.5-flash: 0.075
    gpt-4o: 2.50
    gpt-4o-mini: 0.15
    claude-3-5-sonnet: 3.00
  output:
    gemini-2.0-flash: 0.60
    gemini-1.5-flash: 0.30
    gpt-4o: 10.00
    gpt-4o-mini: 0.60
    claude-3-5-sonnet: 15.00

# Image generation costs (per image)
image_costs:
  gemini-2.5-flash-image: 0.04  # Gemini dedicated image generation model
  dall-e-3: 0.04
  dall-e-2: 0.02

# Video generation costs (per second)
video_costs:
  sora-2: 0.125
  sora-2-pro: 0.50

# Local models configuration
local_models:
  # Recommended models by size (download with: POST /local-models/pull)
  small:  # 2-4GB, fast inference
    - "llama3.2:3b"      # 3B params, ~2GB, good for testing
    - "phi3:mini"        # 3.8B params, ~2.3GB, fast
    - "gemma2:2b"        # 2B params, ~1.6GB, very fast

  medium:  # 4-8GB, balanced performance
    - "llama2:7b"        # 7B params, ~4GB, reliable
    - "mistral:7b"       # 7B params, ~4GB, fast inference
    - "qwen2.5:7b"       # 7B params, ~4.7GB, strong performance
    - "codellama:7b"     # 7B params, ~4GB, code generation

  large:  # 26-50GB, powerful but slow
    - "mixtral:8x7b"     # 47B params, ~26GB, very powerful
    - "qwen2.5:32b"      # 32B params, ~19GB, strong reasoning
    - "llama3.1:70b"     # 70B params, ~40GB, very strong

  xlarge:  # 40GB+, requires significant resources
    - "qwen2.5:72b"      # 72B params, ~41GB, similar to GPT-4 class
    - "llama3.1:405b"    # 405B params, ~231GB, cutting-edge (requires multiple GPUs)

  # Cost per 1M tokens (all FREE for local models!)
  cost:
    input: 0.0
    output: 0.0

# Tool-specific settings (temperature, etc.)
tool_settings:
  character_appearance_analyzer:
    temperature: 0.3
